#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çµ±ä¸€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ãƒ³ãƒŠãƒ¼

ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹çµ±åˆãƒ©ãƒ³ãƒŠãƒ¼ã€‚
ä¸¦åˆ—å®Ÿè¡Œã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã€çµæœåé›†ã‚’ç®¡ç†ã—ã¾ã™ã€‚
"""

import time
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from datetime import datetime
from typing import Any, Dict, List, Optional, Union

import numpy as np

from benchmarks.core.types import (
    BenchmarkConfig,
    BenchmarkResult,
    BenchmarkTarget,
    TimingData,
    BenchmarkMetrics,
)
from benchmarks.core.config import get_config
from benchmarks.core.exceptions import (
    BenchmarkError,
    BenchmarkTimeoutError,
    benchmark_operation,
    get_error_collector,
    get_error_handler,
)
from benchmarks.plugins.base import PluginManager, create_plugin_manager
from engine.core.geometry import Geometry


class UnifiedBenchmarkRunner:
    """çµ±ä¸€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ãƒ³ãƒŠãƒ¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: Optional[BenchmarkConfig] = None):
        self.config = config or get_config()
        self.plugin_manager = create_plugin_manager(self.config)
        self.error_handler = get_error_handler()
        self.error_collector = get_error_collector()
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¨­å®š
        self.error_handler.configure(
            max_errors=self.config.max_errors,
            continue_on_error=self.config.continue_on_error
        )
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®é…å»¶åˆæœŸåŒ–
        self._test_geometries: Optional[Dict[str, Geometry]] = None
    
    @property
    def test_geometries(self) -> Dict[str, Geometry]:
        """ãƒ†ã‚¹ãƒˆã‚¸ã‚ªãƒ¡ãƒˆãƒªã®é…å»¶åˆæœŸåŒ–"""
        if self._test_geometries is None:
            self._test_geometries = self._create_test_geometries()
        return self._test_geometries
    
    def _create_test_geometries(self) -> Dict[str, Geometry]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ†ã‚¹ãƒˆã‚¸ã‚ªãƒ¡ãƒˆãƒªã‚’ä½œæˆ"""
        geometries = {}
        
        # Simple rectangle
        geometries["small"] = self._create_rectangle(1, 1)
        
        # Medium polygon
        geometries["medium"] = self._create_polygon(20)
        
        # Complex shape (multiple circles)
        geometries["large"] = self._create_complex_shape()
        
        return geometries
    
    @staticmethod
    def _create_rectangle(width: float, height: float) -> Geometry:
        """ã‚·ãƒ³ãƒ—ãƒ«ãªé•·æ–¹å½¢ã‚’ä½œæˆ"""
        hw, hh = width / 2, height / 2
        vertices = np.array(
            [[-hw, -hh, 0.0], [hw, -hh, 0.0], [hw, hh, 0.0], [-hw, hh, 0.0], [-hw, -hh, 0.0]], 
            dtype=np.float32
        )
        return Geometry.from_lines([vertices])
    
    @staticmethod
    def _create_polygon(n: int) -> Geometry:
        """nè¾ºã®æ­£å¤šè§’å½¢ã‚’ä½œæˆ"""
        angles = np.linspace(0, 2 * np.pi, n + 1)
        vertices = np.column_stack([np.cos(angles), np.sin(angles), np.zeros_like(angles)]).astype(np.float32)
        return Geometry.from_lines([vertices])
    
    def _create_complex_shape(self) -> Geometry:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ã®è¤‡é›‘ãªå½¢çŠ¶ã‚’ä½œæˆï¼ˆè¤‡æ•°ã®å††ã‚’çµåˆï¼‰"""
        circles = []
        for i in range(10):
            radius = 1.0 + i * 0.1
            angles = np.linspace(0, 2 * np.pi, 64 + 1)
            vertices = np.column_stack(
                [radius * np.cos(angles), radius * np.sin(angles), np.zeros_like(angles)]
            ).astype(np.float32)
            circles.append(vertices)
        return Geometry.from_lines(circles)
    
    def run_all_benchmarks(self) -> Dict[str, BenchmarkResult]:
        """ã™ã¹ã¦ã®ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"""
        all_results = {}
        
        print(f"Starting unified benchmark run at {datetime.now()}")
        print("=" * 60)
        
        # ã™ã¹ã¦ã®ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’å–å¾—
        all_targets = self.plugin_manager.get_all_targets()
        
        total_targets = sum(len(targets) for targets in all_targets.values())
        print(f"Found {total_targets} benchmark targets across {len(all_targets)} plugins")
        
        # ãƒ—ãƒ©ã‚°ã‚¤ãƒ³åˆ¥ã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        for plugin_name, targets in all_targets.items():
            if not targets:
                print(f"\nSkipping {plugin_name}: no targets found")
                continue
            
            print(f"\n--- Benchmarking {plugin_name} ({len(targets)} targets) ---")
            
            if self.config.parallel:
                plugin_results = self._run_plugin_parallel(plugin_name, targets)
            else:
                plugin_results = self._run_plugin_sequential(plugin_name, targets)
            
            all_results.update(plugin_results)
        
        # ã‚¨ãƒ©ãƒ¼è¦ç´„ã‚’è¡¨ç¤º
        if self.error_collector.has_errors() or self.error_collector.has_warnings():
            print("\n" + "=" * 60)
            print("ERROR SUMMARY")
            print("=" * 60)
            print(self.error_collector.generate_report())
        
        print(f"\nBenchmark completed at {datetime.now()}")
        print(f"Total results: {len(all_results)}")
        
        # è‡ªå‹•ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
        if all_results and self.config.generate_charts:
            self._generate_auto_visualization(all_results)
        
        return all_results
    
    def _run_plugin_sequential(self, plugin_name: str, targets: List[BenchmarkTarget]) -> Dict[str, BenchmarkResult]:
        """ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’é †æ¬¡å®Ÿè¡Œ"""
        results = {}
        
        for i, target in enumerate(targets, 1):
            print(f"  [{i}/{len(targets)}] {target.name}...", end=" ", flush=True)
            
            try:
                result = self.benchmark_target(target)
                results[target.name] = result
                
                if result["success"]:
                    avg_time = np.mean(list(result["average_times"].values())) if result["average_times"] else 0
                    if avg_time > 0:
                        fps = 1.0 / avg_time
                        print(f"âœ“ ({fps:.1f} fps)")
                    else:
                        print("âœ“ (instant)")
                else:
                    print(f"âœ— ({result.get('error', 'unknown error')})")
                    
            except Exception as e:
                print(f"âœ— (exception: {e})")
                # ã‚¨ãƒ©ãƒ¼ã‚’åé›†
                error = BenchmarkError(f"Unhandled exception in {target.name}: {e}", module_name=target.name)
                self.error_collector.add_error(error)
        
        return results
    
    def _run_plugin_parallel(self, plugin_name: str, targets: List[BenchmarkTarget]) -> Dict[str, BenchmarkResult]:
        """ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œ"""
        results = {}
        max_workers = self.config.max_workers or min(len(targets), 4)
        
        print(f"  Running {len(targets)} targets in parallel (max_workers={max_workers})")
        
        # å½¢çŠ¶ç”Ÿæˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã€ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã¯ãƒ—ãƒ­ã‚»ã‚¹ãƒ—ãƒ¼ãƒ«ã‚’ä½¿ç”¨
        if plugin_name == "shapes":
            executor_class = ThreadPoolExecutor  # å½¢çŠ¶ç”Ÿæˆã¯I/Oãƒã‚¦ãƒ³ãƒ‰
        else:
            executor_class = ProcessPoolExecutor  # ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã¯CPUãƒã‚¦ãƒ³ãƒ‰
        
        with executor_class(max_workers=max_workers) as executor:
            # å…¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œ
            future_to_target = {
                executor.submit(self._benchmark_target_isolated, target): target
                for target in targets
            }
            
            completed = 0
            for future in as_completed(future_to_target):
                target = future_to_target[future]
                completed += 1
                
                try:
                    result = future.result(timeout=self.config.timeout_seconds)
                    results[target.name] = result
                    
                    status = "âœ“" if result["success"] else "âœ—"
                    print(f"  [{completed}/{len(targets)}] {target.name} {status}")
                    
                except Exception as e:
                    print(f"  [{completed}/{len(targets)}] {target.name} âœ— (error: {e})")
                    error = BenchmarkError(f"Unhandled exception in {target.name}: {e}", module_name=target.name)
                    self.error_collector.add_error(error)
        
        return results
    
    def benchmark_target(self, target: BenchmarkTarget) -> BenchmarkResult:
        """å˜ä¸€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"""
        return self._benchmark_target_isolated(target)
    
    def _benchmark_target_isolated(self, target: BenchmarkTarget) -> BenchmarkResult:
        """åˆ†é›¢ã•ã‚ŒãŸç’°å¢ƒã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆä¸¦åˆ—å®Ÿè¡Œå¯¾å¿œï¼‰"""
        result = BenchmarkResult(
            module=target.name,
            timestamp=datetime.now().isoformat(),
            success=False,
            error=None,
            status="failed",
            timings={},
            average_times={},
            metrics={}
        )
        
        try:
            with benchmark_operation(f"benchmark_{target.name}", target.name):
                # å¯¾è±¡ã®ç‰¹æ€§ã‚’åˆ†æ
                plugin = self._get_plugin_for_target(target)
                if plugin:
                    features = plugin.analyze_target_features(target)
                    result["metrics"].update({
                        "has_njit": features.get("has_njit", False),
                        "has_cache": features.get("has_cache", False),
                        "function_count": features.get("function_count", 1),
                    })
                
                # å½¢çŠ¶ç”Ÿæˆã‹ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã‹ã§å‡¦ç†ã‚’åˆ†å²
                if self._is_shape_target(target):
                    # å½¢çŠ¶ç”Ÿæˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
                    times = self._benchmark_shape_generation(target)
                    if times:
                        result["timings"]["generation"] = times
                        result["average_times"]["generation"] = float(np.mean(times))
                else:
                    # ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆè¤‡æ•°ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºï¼‰
                    for size_name, test_geom in self.test_geometries.items():
                        times = self._benchmark_effect_application(target, test_geom)
                        if times:
                            result["timings"][size_name] = times
                            result["average_times"][size_name] = float(np.mean(times))
                
                # æˆåŠŸåˆ¤å®š
                if result["timings"]:
                    result["success"] = True
                    result["status"] = "success"
                    
                    # å…¨ä½“çµ±è¨ˆã‚’è¨ˆç®—
                    all_times = [t for times in result["timings"].values() for t in times]
                    result["metrics"]["total_measurements"] = len(all_times)
                    result["metrics"]["overall_avg_time"] = float(np.mean(all_times))
                    result["metrics"]["overall_std_time"] = float(np.std(all_times))
                else:
                    result["error"] = "No successful measurements"
                    result["status"] = "error"
        
        except BenchmarkTimeoutError as e:
            result["error"] = f"Timeout: {str(e)}"
            result["status"] = "timeout"
        except Exception as e:
            result["error"] = f"Error: {str(e)}"
            result["status"] = "error"
            result["metrics"]["error_type"] = type(e).__name__
        
        return result
    
    def _benchmark_shape_generation(self, target: BenchmarkTarget) -> Optional[List[float]]:
        """å½¢çŠ¶ç”Ÿæˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(self.config.warmup_runs):
            try:
                target.execute()
            except Exception:
                return None
        
        # æ¸¬å®š
        times = []
        for _ in range(self.config.measurement_runs):
            try:
                start_time = time.perf_counter()
                result = target.execute()
                end_time = time.perf_counter()
                
                # çµæœã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                if hasattr(result, 'coords') and len(result.coords) > 0:
                    times.append(end_time - start_time)
                
            except Exception:
                return None
        
        return times if times else None
    
    def _benchmark_effect_application(self, target: BenchmarkTarget, test_geom: Geometry) -> Optional[List[float]]:
        """ã‚¨ãƒ•ã‚§ã‚¯ãƒˆé©ç”¨ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(self.config.warmup_runs):
            try:
                target.execute(test_geom)
            except Exception:
                return None
        
        # æ¸¬å®š
        times = []
        for _ in range(self.config.measurement_runs):
            try:
                start_time = time.perf_counter()
                result = target.execute(test_geom)
                end_time = time.perf_counter()
                
                # çµæœã®å¦¥å½“æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                if hasattr(result, 'coords') and len(result.coords) > 0:
                    times.append(end_time - start_time)
                
            except Exception:
                return None
        
        return times if times else None
    
    def _is_shape_target(self, target: BenchmarkTarget) -> bool:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒå½¢çŠ¶ç”Ÿæˆã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åˆ¤å®š
        if hasattr(target, 'metadata'):
            return target.metadata.get('shape_type') is not None
        
        # åå‰ã‹ã‚‰åˆ¤å®š
        shape_indicators = [
            'polygon', 'sphere', 'cylinder', 'cone', 'torus', 'capsule',
            'polyhedron', 'lissajous', 'attractor', 'text', 'asemic_glyph', 'grid'
        ]
        return any(indicator in target.name for indicator in shape_indicators)
    
    def _get_plugin_for_target(self, target: BenchmarkTarget) -> Optional[Any]:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«å¯¾å¿œã™ã‚‹ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’å–å¾—"""
        for plugin in self.plugin_manager.get_all_plugins():
            targets = plugin.get_targets()
            if any(t.name == target.name for t in targets):
                return plugin
        return None
    
    def run_specific_targets(self, target_names: List[str]) -> Dict[str, BenchmarkResult]:
        """ç‰¹å®šã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã¿ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"""
        all_targets = self.plugin_manager.get_all_targets()
        
        # åå‰ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æ¤œç´¢
        selected_targets = []
        for plugin_targets in all_targets.values():
            for target in plugin_targets:
                if target.name in target_names:
                    selected_targets.append(target)
        
        if not selected_targets:
            print(f"No targets found matching: {target_names}")
            return {}
        
        print(f"Running {len(selected_targets)} specific targets")
        
        # é †æ¬¡å®Ÿè¡Œ
        results = {}
        for target in selected_targets:
            print(f"Benchmarking {target.name}...", end=" ", flush=True)
            result = self.benchmark_target(target)
            results[target.name] = result
            
            status = "âœ“" if result["success"] else "âœ—"
            print(status)
        
        # è‡ªå‹•ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
        if results and self.config.generate_charts:
            self._generate_auto_visualization(results)
        
        return results
    
    def get_available_targets(self) -> Dict[str, List[str]]:
        """åˆ©ç”¨å¯èƒ½ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¸€è¦§ã‚’å–å¾—"""
        all_targets = self.plugin_manager.get_all_targets()
        return {
            plugin_name: [target.name for target in targets]
            for plugin_name, targets in all_targets.items()
        }
    
    def _generate_auto_visualization(self, results: Dict[str, BenchmarkResult]) -> None:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®è‡ªå‹•ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³"""
        try:
            print("\n--- Generating Visualizations ---")
            
            from benchmarks.visualization.charts import ChartGenerator
            from benchmarks.visualization.reports import ReportGenerator
            
            # ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆ
            chart_generator = ChartGenerator(self.config.output_dir)
            chart_paths = []
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
            try:
                bar_chart = chart_generator.create_performance_chart(results, "bar")
                chart_paths.append(bar_chart)
                print(f"ğŸ“Š Bar chart: {bar_chart}")
            except Exception as e:
                print(f"Warning: Failed to create bar chart: {e}")
            
            # ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæˆåŠŸã—ãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒè¤‡æ•°ã‚ã‚‹å ´åˆã®ã¿ï¼‰
            successful_count = sum(1 for r in results.values() if r["success"])
            if successful_count > 1:
                try:
                    box_chart = chart_generator.create_performance_chart(results, "box")
                    chart_paths.append(box_chart)
                    print(f"ğŸ“¦ Box plot: {box_chart}")
                except Exception as e:
                    print(f"Warning: Failed to create box plot: {e}")
            
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆæˆåŠŸã—ãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒè¤‡æ•°ã‚ã‚‹å ´åˆã®ã¿ï¼‰
            if successful_count > 2:
                try:
                    heatmap = chart_generator.create_performance_chart(results, "heatmap")
                    chart_paths.append(heatmap)
                    print(f"ğŸ”¥ Heatmap: {heatmap}")
                except Exception as e:
                    print(f"Warning: Failed to create heatmap: {e}")
            
            # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            try:
                report_generator = ReportGenerator(self.config.output_dir)
                html_report = report_generator.generate_html_report(results, chart_paths)
                print(f"ğŸ“„ HTML report: {html_report}")
            except Exception as e:
                print(f"Warning: Failed to create HTML report: {e}")
            
            # Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            try:
                md_report = report_generator.generate_markdown_report(results, chart_paths)
                print(f"ğŸ“ Markdown report: {md_report}")
            except Exception as e:
                print(f"Warning: Failed to create Markdown report: {e}")
            
            print("--- Visualization Complete ---")
            
        except Exception as e:
            print(f"Error during visualization: {e}")
            # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒ©ãƒ¼ã¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã«å½±éŸ¿ã—ãªã„ã®ã§ç¶šè¡Œ


# ä¾¿åˆ©é–¢æ•°
def create_runner(config: Optional[BenchmarkConfig] = None) -> UnifiedBenchmarkRunner:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ãƒ³ãƒŠãƒ¼ã‚’ä½œæˆã™ã‚‹ä¾¿åˆ©é–¢æ•°"""
    return UnifiedBenchmarkRunner(config)


def run_benchmarks(config: Optional[BenchmarkConfig] = None) -> Dict[str, BenchmarkResult]:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ä¾¿åˆ©é–¢æ•°"""
    runner = create_runner(config)
    return runner.run_all_benchmarks()